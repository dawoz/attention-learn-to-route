diff --git a/nets/attention_model.py b/nets/attention_model.py
index 02790fa..e3fa645 100644
--- a/nets/attention_model.py
+++ b/nets/attention_model.py
@@ -545,7 +545,7 @@ class AttentionModelCustom(AttentionModel):
         )
 
         self.num_dist = num_dist
-        self.init_embed = nn.Linear(2+num_dist, embedding_dim)
+        self.init_embed = nn.Linear(num_dist if num_dist > 0 else 2, embedding_dim)
         self.embedder = GraphAttentionEncoderCustom(
             n_heads=n_heads,
             embed_dim=embedding_dim,
@@ -562,9 +562,6 @@ class AttentionModelCustom(AttentionModel):
         using DataParallel as the results may be of different lengths on different GPUs
         :return:
         """
-
-        input = input[:, :, :self.num_dist+2]
-
         if self.checkpoint_encoder and self.training:  # Only checkpoint if we need gradients
             embeddings, _ = checkpoint(self.embedder, self._init_embed(input))
         else:
diff --git a/problems/tsp/problem_tsp.py b/problems/tsp/problem_tsp.py
index 51b136c..1b0a789 100644
--- a/problems/tsp/problem_tsp.py
+++ b/problems/tsp/problem_tsp.py
@@ -2,7 +2,7 @@ from torch.utils.data import Dataset
 import torch
 import os
 import pickle
-from problems.tsp.state_tsp import StateTSP
+from problems.tsp.state_tsp import StateTSP, StateTSPDist
 from utils.beam_search import beam_search
 import torch.nn.functional as F
 
@@ -78,7 +78,7 @@ class TSPDataset(Dataset):
         return self.data[idx]
 
 
-class TSPDist(TSP):
+class TSPDist_(TSP):
 
     NAME = 'tsp_dist'
 
@@ -98,7 +98,7 @@ class TSPDist(TSP):
 
     @staticmethod
     def make_dataset(*args, **kwargs):
-        return TSPDistDataset(*args, **kwargs)
+        return TSPDist_Dataset(*args, **kwargs)
 
 
 def pdist(x):
@@ -110,9 +110,9 @@ def pdist(x):
     return m
 
 
-class TSPDistDataset(Dataset):
+class TSPDist_Dataset(Dataset):
     def __init__(self, filename=None, size=50, num_samples=1000000, offset=0, distribution=None):
-        super(TSPDistDataset, self).__init__()
+        super(TSPDist_Dataset, self).__init__()
 
         self.data_set = []
         if filename is not None:
@@ -133,4 +133,59 @@ class TSPDistDataset(Dataset):
         return self.size
 
     def __getitem__(self, idx):
-        return torch.cat([self.data[idx],self.distances[idx]],dim=1)
\ No newline at end of file
+        return torch.cat([self.data[idx],self.distances[idx]],dim=1)
+
+
+
+class TSPDist(TSP):
+
+    NAME = 'tsp_dist'
+
+    @staticmethod
+    def get_costs(dataset, pi):
+        # pi: matrix (batch_size, graph_size)
+
+        # Check that tours are valid, i.e. contain 0 to n -1
+        assert (
+            torch.arange(pi.size(1), out=pi.data.new()).view(1, -1).expand_as(pi) ==
+            pi.data.sort(1)[0]
+        ).all(), "Invalid tour"
+
+        a = torch.arange(pi.shape[1])
+        idx = torch.stack((a, a.roll(-1,0)))  # neighbor cities
+        return dataset[:,idx[0],idx[1]].sum(1), None
+
+    @staticmethod
+    def make_state(*args, **kwargs):
+        return StateTSPDist.initialize(*args, **kwargs)
+
+    @staticmethod
+    def make_dataset(*args, **kwargs):
+        return TSPDistDataset(*args, **kwargs)
+
+
+class TSPDistDataset(Dataset):
+    def __init__(self, filename=None, size=50, num_samples=1000000, offset=0, distribution=None):
+        super(TSPDistDataset, self).__init__()
+
+        self.data_set = []
+        if filename is not None:
+            assert os.path.splitext(filename)[1] == '.pkl'
+
+            with open(filename, 'rb') as f:
+                data = pickle.load(f)
+                self.data = [torch.FloatTensor(row) for row in (data[offset:offset+num_samples])]
+        else:
+            # Sample distances in [0, 1]
+            self.data = []
+            for i in range(num_samples):
+                r = torch.FloatTensor(size, size).uniform_(0, 1).triu(1)
+                self.data.append((r + r.t()))
+
+        self.size = len(self.data)
+
+    def __len__(self):
+        return self.size
+
+    def __getitem__(self, idx):
+        return self.data[idx]
\ No newline at end of file
diff --git a/problems/tsp/state_tsp.py b/problems/tsp/state_tsp.py
index 4730605..20852b2 100644
--- a/problems/tsp/state_tsp.py
+++ b/problems/tsp/state_tsp.py
@@ -131,3 +131,133 @@ class StateTSP(NamedTuple):
 
     def construct_solutions(self, actions):
         return actions
+
+
+class StateTSPDist(NamedTuple):
+    # Fixed input
+    loc: torch.Tensor
+    dist: torch.Tensor
+
+    # If this state contains multiple copies (i.e. beam search) for the same instance, then for memory efficiency
+    # the loc and dist tensors are not kept multiple times, so we need to use the ids to index the correct rows.
+    ids: torch.Tensor  # Keeps track of original fixed data index of rows
+
+    # State
+    first_a: torch.Tensor
+    prev_a: torch.Tensor
+    visited_: torch.Tensor  # Keeps track of nodes that have been visited
+    lengths: torch.Tensor
+    cur_coord: torch.Tensor
+    i: torch.Tensor  # Keeps track of step
+
+    @property
+    def visited(self):
+        if self.visited_.dtype == torch.uint8:
+            return self.visited_
+        else:
+            return mask_long2bool(self.visited_, n=self.loc.size(-2))
+
+    def __getitem__(self, key):
+        assert torch.is_tensor(key) or isinstance(key, slice)  # If tensor, idx all tensors by this tensor:
+        return self._replace(
+            ids=self.ids[key],
+            first_a=self.first_a[key],
+            prev_a=self.prev_a[key],
+            visited_=self.visited_[key],
+            lengths=self.lengths[key],
+            cur_coord=self.cur_coord[key] if self.cur_coord is not None else None,
+        )
+
+    @staticmethod
+    def initialize(loc, visited_dtype=torch.uint8):
+
+        batch_size, n_loc, _ = loc.size()
+        prev_a = torch.zeros(batch_size, 1, dtype=torch.long, device=loc.device)
+        return StateTSPDist(
+            loc=loc,
+            dist=(loc[:, :, None, :] - loc[:, None, :, :]).norm(p=2, dim=-1),
+            ids=torch.arange(batch_size, dtype=torch.int64, device=loc.device)[:, None],  # Add steps dimension
+            first_a=prev_a,
+            prev_a=prev_a,
+            # Keep visited with depot so we can scatter efficiently (if there is an action for depot)
+            visited_=(  # Visited as mask is easier to understand, as long more memory efficient
+                torch.zeros(
+                    batch_size, 1, n_loc,
+                    dtype=torch.uint8, device=loc.device
+                )
+                if visited_dtype == torch.uint8
+                else torch.zeros(batch_size, 1, (n_loc + 63) // 64, dtype=torch.int64, device=loc.device)  # Ceil
+            ),
+            lengths=torch.zeros(batch_size, 1, device=loc.device),
+            cur_coord=None,
+            i=torch.zeros(1, dtype=torch.int64, device=loc.device)  # Vector with length num_steps
+        )
+
+    def get_final_cost(self):
+
+        assert self.all_finished()
+        # assert self.visited_.
+
+        return self.lengths + (self.loc[self.ids, self.first_a, :] - self.cur_coord).norm(p=2, dim=-1)
+
+    def update(self, selected):
+
+        # Update the state
+        prev_a = selected[:, None]  # Add dimension for step
+
+        # Add the length
+        # cur_coord = self.loc.gather(
+        #     1,
+        #     selected[:, None, None].expand(selected.size(0), 1, self.loc.size(-1))
+        # )[:, 0, :]
+        cur_coord = self.loc[self.ids, prev_a]
+        lengths = self.lengths
+        if self.cur_coord is not None:  # Don't add length for first action (selection of start node)
+            lengths = self.lengths + (cur_coord - self.cur_coord).norm(p=2, dim=-1)  # (batch_dim, 1)
+
+        # Update should only be called with just 1 parallel step, in which case we can check this way if we should update
+        first_a = prev_a if self.i.item() == 0 else self.first_a
+
+        if self.visited_.dtype == torch.uint8:
+            # Add one dimension since we write a single value
+            visited_ = self.visited_.scatter(-1, prev_a[:, :, None], 1)
+        else:
+            visited_ = mask_long_scatter(self.visited_, prev_a)
+
+        return self._replace(first_a=first_a, prev_a=prev_a, visited_=visited_,
+                             lengths=lengths, cur_coord=cur_coord, i=self.i + 1)
+
+    def all_finished(self):
+        # Exactly n steps
+        return self.i.item() >= self.loc.size(-2)
+
+    def get_current_node(self):
+        return self.prev_a
+
+    def get_mask(self):
+        return self.visited > 0  # Hacky way to return bool or uint8 depending on pytorch version
+
+    def get_nn(self, k=None):
+        # Insert step dimension
+        # Nodes already visited get inf so they do not make it
+        if k is None:
+            k = self.loc.size(-2) - self.i.item()  # Number of remaining
+        return (self.dist[self.ids, :, :] + self.visited.float()[:, :, None, :] * 1e6).topk(k, dim=-1, largest=False)[1]
+
+    def get_nn_current(self, k=None):
+        assert False, "Currently not implemented, look into which neighbours to use in step 0?"
+        # Note: if this is called in step 0, it will have k nearest neighbours to node 0, which may not be desired
+        # so it is probably better to use k = None in the first iteration
+        if k is None:
+            k = self.loc.size(-2)
+        k = min(k, self.loc.size(-2) - self.i.item())  # Number of remaining
+        return (
+            self.dist[
+                self.ids,
+                self.prev_a
+            ] +
+            self.visited.float() * 1e6
+        ).topk(k, dim=-1, largest=False)[1]
+
+    def construct_solutions(self, actions):
+        return actions
